{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215b9b22",
   "metadata": {},
   "source": [
    "# Model Training & Comparison (ANN vs CNN)\n",
    "\n",
    "This notebook trains and compares simple **ANN** and **CNN** models on:\n",
    "\n",
    "- **MNIST** (ANN & CNN)\n",
    "- **CIFAR-10** (CNN)\n",
    "- **Titanic** (ANN)\n",
    "\n",
    "You can toggle sections to run the experiments you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bd7a7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4a6b3",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c8c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title_prefix=''):\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(history.history.get('accuracy', []), label='train_acc')\n",
    "    plt.plot(history.history.get('val_accuracy', []), label='val_acc')\n",
    "    plt.title(f\"{title_prefix} Accuracy\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history.get('loss', []), label='train_loss')\n",
    "    plt.plot(history.history.get('val_loss', []), label='val_loss')\n",
    "    plt.title(f\"{title_prefix} Loss\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, is_logits=False):\n",
    "    preds = model.predict(X_test)\n",
    "    if preds.ndim > 1 and preds.shape[-1] > 1:\n",
    "        y_pred = np.argmax(preds, axis=1)\n",
    "    else:\n",
    "        y_pred = (preds.flatten() > 0.5).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', acc)\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd26ab",
   "metadata": {},
   "source": [
    "## Section A: MNIST — ANN vs CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99591b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype('float32')/255.0\n",
    "X_test = X_test.astype('float32')/255.0\n",
    "\n",
    "# Prepare one-hot labels for training (sparse labels are also fine with sparse_categorical_crossentropy)\n",
    "num_classes = 10\n",
    "\n",
    "# -------- ANN --------\n",
    "ann = models.Sequential([\n",
    "    layers.Input(shape=(28,28)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "ann.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist_ann = ann.fit(X_train, y_train, validation_split=0.1, epochs=5, batch_size=128, verbose=1)\n",
    "plot_history(hist_ann, 'MNIST ANN')\n",
    "acc_ann = evaluate_model(ann, X_test, y_test)\n",
    "\n",
    "# -------- CNN --------\n",
    "X_train_cnn = np.expand_dims(X_train, -1)\n",
    "X_test_cnn = np.expand_dims(X_test, -1)\n",
    "\n",
    "cnn = models.Sequential([\n",
    "    layers.Input(shape=(28,28,1)),\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist_cnn = cnn.fit(X_train_cnn, y_train, validation_split=0.1, epochs=5, batch_size=128, verbose=1)\n",
    "plot_history(hist_cnn, 'MNIST CNN')\n",
    "acc_cnn = evaluate_model(cnn, X_test_cnn, y_test)\n",
    "\n",
    "print('MNIST Results -> ANN:', acc_ann, '| CNN:', acc_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e496e",
   "metadata": {},
   "source": [
    "## Section B: CIFAR-10 — CNN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb36d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "(X_train_c, y_train_c), (X_test_c, y_test_c) = cifar10.load_data()\n",
    "y_train_c = y_train_c.flatten()\n",
    "y_test_c = y_test_c.flatten()\n",
    "\n",
    "# Normalize\n",
    "X_train_c = X_train_c.astype('float32')/255.0\n",
    "X_test_c = X_test_c.astype('float32')/255.0\n",
    "\n",
    "num_classes = 10\n",
    "cifar_cnn = models.Sequential([\n",
    "    layers.Input(shape=(32,32,3)),\n",
    "    layers.Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "cifar_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "hist_cifar = cifar_cnn.fit(X_train_c, y_train_c, validation_split=0.1, epochs=5, batch_size=128, verbose=1)\n",
    "plot_history(hist_cifar, 'CIFAR-10 CNN')\n",
    "acc_cifar = evaluate_model(cifar_cnn, X_test_c, y_test_c)\n",
    "print('CIFAR-10 Result -> CNN:', acc_cifar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa67df",
   "metadata": {},
   "source": [
    "## Section C: Titanic — ANN baseline\n",
    "\n",
    "Place `train.csv` at `../datasets/titanic/train.csv`. This is a quick baseline using a small ANN on tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../datasets/titanic/train.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.dropna(subset=['Age','Fare','Sex','Pclass','Survived'])\n",
    "    df['Sex'] = df['Sex'].map({'male':0, 'female':1})\n",
    "    X = df[['Pclass','Sex','Age','Fare']].values.astype('float32')\n",
    "    y = df['Survived'].values.astype('int32')\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    titanic_ann = models.Sequential([\n",
    "        layers.Input(shape=(X.shape[1],)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    titanic_ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    hist_titanic = titanic_ann.fit(X_train_t, y_train_t, validation_split=0.1, epochs=10, batch_size=32, verbose=1)\n",
    "    plot_history(hist_titanic, 'Titanic ANN')\n",
    "    acc_titanic = evaluate_model(titanic_ann, X_test_t, y_test_t)\n",
    "    print('Titanic Result -> ANN:', acc_titanic)\n",
    "else:\n",
    "    print('Titanic CSV not found at', csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71871bf",
   "metadata": {},
   "source": [
    "## Summary Cell — Compare Accuracies (if you ran multiple sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efec302",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "try:\n",
    "    summary['MNIST_ANN'] = float(acc_ann)\n",
    "    summary['MNIST_CNN'] = float(acc_cnn)\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    summary['CIFAR10_CNN'] = float(acc_cifar)\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    summary['Titanic_ANN'] = float(acc_titanic)\n",
    "except Exception:\n",
    "    pass\n",
    "print('Accuracy Summary:')\n",
    "for k,v in summary.items():\n",
    "    print(k, ':', v)\n",
    "\n",
    "if summary:\n",
    "    plt.figure()\n",
    "    plt.bar(list(summary.keys()), list(summary.values()))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
